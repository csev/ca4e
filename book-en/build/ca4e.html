<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Charles R. Severance" />
  <meta name="dcterms.date" content="2026-01-01" />
  <title>Computer Architecture for Everybody</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="styles/book.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Computer Architecture for Everybody</h1>
<p class="subtitle">From Stonehenge to Silicon</p>
<p class="author">Charles R. Severance</p>
<p class="date">2026</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a
href="#from-stonehenge-to-silicon-why-computers-began-with-numbers"
id="toc-from-stonehenge-to-silicon-why-computers-began-with-numbers">From
Stonehenge to Silicon: Why Computers Began with Numbers</a>
<ul>
<li><a href="#curved-motion-and-the-need-for-prediction"
id="toc-curved-motion-and-the-need-for-prediction">Curved Motion and the
Need for Prediction</a></li>
<li><a href="#astronomy-and-physical-data-tables"
id="toc-astronomy-and-physical-data-tables">Astronomy and Physical Data
Tables</a></li>
<li><a href="#stonehenge-as-a-measuring-device"
id="toc-stonehenge-as-a-measuring-device">Stonehenge as a Measuring
Device</a></li>
<li><a href="#continuous-representations-of-number"
id="toc-continuous-representations-of-number">Continuous Representations
of Number</a></li>
<li><a href="#gears-as-models-of-the-solar-system"
id="toc-gears-as-models-of-the-solar-system">Gears as Models of the
Solar System</a></li>
<li><a href="#practical-analog-computing"
id="toc-practical-analog-computing">Practical Analog Computing</a></li>
<li><a href="#discrete-states-and-digital-devices"
id="toc-discrete-states-and-digital-devices">Discrete States and Digital
Devices</a></li>
<li><a href="#iteration-and-mechanical-automation"
id="toc-iteration-and-mechanical-automation">Iteration and Mechanical
Automation</a></li>
<li><a href="#human-computers-and-early-programming"
id="toc-human-computers-and-early-programming">Human Computers and Early
Programming</a></li>
<li><a href="#from-mechanical-to-electronic-switching"
id="toc-from-mechanical-to-electronic-switching">From Mechanical to
Electronic Switching</a></li>
<li><a href="#why-architecture-begins-with-numbers"
id="toc-why-architecture-begins-with-numbers">Why Architecture Begins
with Numbers</a></li>
<li><a href="#what-comes-next" id="toc-what-comes-next">What Comes
Next</a></li>
</ul></li>
<li><a
href="#from-tubes-to-transistors-how-solid-state-electronics-changed-everything"
id="toc-from-tubes-to-transistors-how-solid-state-electronics-changed-everything">From
Tubes to Transistors: How Solid-State Electronics Changed Everything</a>
<ul>
<li><a href="#electronic-switching-with-vacuum-tubes"
id="toc-electronic-switching-with-vacuum-tubes">Electronic Switching
with Vacuum Tubes</a></li>
<li><a href="#the-limits-of-tube-based-computing"
id="toc-the-limits-of-tube-based-computing">The Limits of Tube-Based
Computing</a></li>
<li><a href="#semiconductors-and-controlled-conductivity"
id="toc-semiconductors-and-controlled-conductivity">Semiconductors and
Controlled Conductivity</a></li>
<li><a href="#transistors-as-electronic-switches"
id="toc-transistors-as-electronic-switches">Transistors as Electronic
Switches</a></li>
<li><a href="#analog-amplifiers-and-digital-switching"
id="toc-analog-amplifiers-and-digital-switching">Analog Amplifiers and
Digital Switching</a></li>
<li><a href="#nmos-pmos-and-complementary-pairs"
id="toc-nmos-pmos-and-complementary-pairs">NMOS, PMOS, and Complementary
Pairs</a></li>
<li><a href="#from-transistors-to-logic-gates"
id="toc-from-transistors-to-logic-gates">From Transistors to Logic
Gates</a></li>
<li><a href="#building-complexity-from-simple-gates"
id="toc-building-complexity-from-simple-gates">Building Complexity from
Simple Gates</a></li>
<li><a href="#why-cmos-made-modern-computers-possible"
id="toc-why-cmos-made-modern-computers-possible">Why CMOS Made Modern
Computers Possible</a></li>
<li><a href="#summary-solid-state-foundations-of-digital-logic"
id="toc-summary-solid-state-foundations-of-digital-logic">Summary:
Solid-State Foundations of Digital Logic</a></li>
<li><a href="#what-comes-next-1" id="toc-what-comes-next-1">What Comes
Next</a></li>
</ul></li>
<li><a
href="#very-large-scale-integration-from-individual-devices-to-entire-chips"
id="toc-very-large-scale-integration-from-individual-devices-to-entire-chips">Very
Large Scale Integration: From Individual Devices to Entire Chips</a>
<ul>
<li><a href="#from-discrete-components-to-integrated-circuits"
id="toc-from-discrete-components-to-integrated-circuits">From Discrete
Components to Integrated Circuits</a></li>
<li><a href="#planar-transistors-and-two-dimensional-manufacturing"
id="toc-planar-transistors-and-two-dimensional-manufacturing">Planar
Transistors and Two-Dimensional Manufacturing</a></li>
<li><a href="#wafers-masks-and-photolithography"
id="toc-wafers-masks-and-photolithography">Wafers, Masks, and
Photolithography</a></li>
<li><a href="#growth-of-transistor-density-over-time"
id="toc-growth-of-transistor-density-over-time">Growth of Transistor
Density Over Time</a></li>
<li><a href="#designing-with-layout-not-wires"
id="toc-designing-with-layout-not-wires">Designing with Layout, Not
Wires</a></li>
<li><a href="#from-layout-to-logic-gates"
id="toc-from-layout-to-logic-gates">From Layout to Logic Gates</a></li>
<li><a href="#design-tools-and-educational-models"
id="toc-design-tools-and-educational-models">Design Tools and
Educational Models</a></li>
<li><a href="#why-integration-changes-architecture"
id="toc-why-integration-changes-architecture">Why Integration Changes
Architecture</a></li>
<li><a href="#summary-manufacturing-enables-abstraction"
id="toc-summary-manufacturing-enables-abstraction">Summary:
Manufacturing Enables Abstraction</a></li>
<li><a href="#what-comes-next-2" id="toc-what-comes-next-2">What Comes
Next</a></li>
</ul></li>
<li><a href="#digital-logic-building-computation-from-gates"
id="toc-digital-logic-building-computation-from-gates">Digital Logic:
Building Computation from Gates</a>
<ul>
<li><a href="#computers-as-interconnected-components"
id="toc-computers-as-interconnected-components">Computers as
Interconnected Components</a></li>
<li><a href="#what-the-cpu-does" id="toc-what-the-cpu-does">What the CPU
Does</a></li>
<li><a href="#logic-gates-as-building-blocks"
id="toc-logic-gates-as-building-blocks">Logic Gates as Building
Blocks</a></li>
<li><a href="#representing-numbers-in-binary"
id="toc-representing-numbers-in-binary">Representing Numbers in
Binary</a></li>
<li><a href="#adding-numbers-with-gates-half-adders"
id="toc-adding-numbers-with-gates-half-adders">Adding Numbers with
Gates: Half Adders</a></li>
<li><a href="#full-adders-and-multi-bit-addition"
id="toc-full-adders-and-multi-bit-addition">Full Adders and Multi-Bit
Addition</a></li>
<li><a href="#storing-data-with-feedback"
id="toc-storing-data-with-feedback">Storing Data with Feedback</a></li>
<li><a href="#clocked-storage-gated-d-latches"
id="toc-clocked-storage-gated-d-latches">Clocked Storage: Gated D
Latches</a></li>
<li><a href="#registers-from-multiple-latches"
id="toc-registers-from-multiple-latches">Registers from Multiple
Latches</a></li>
<li><a href="#tools-for-exploring-digital-logic"
id="toc-tools-for-exploring-digital-logic">Tools for Exploring Digital
Logic</a></li>
<li><a href="#summary-from-gates-to-computation"
id="toc-summary-from-gates-to-computation">Summary: From Gates to
Computation</a></li>
<li><a href="#what-comes-next-3" id="toc-what-comes-next-3">What Comes
Next</a></li>
</ul></li>
<li><a href="#clocked-circuits-coordinating-computation-over-time"
id="toc-clocked-circuits-coordinating-computation-over-time">Clocked
Circuits: Coordinating Computation Over Time</a>
<ul>
<li><a href="#propagation-delay-and-circuit-length"
id="toc-propagation-delay-and-circuit-length">Propagation Delay and
Circuit Length</a></li>
<li><a href="#clock-rate-and-system-timing"
id="toc-clock-rate-and-system-timing">Clock Rate and System
Timing</a></li>
<li><a href="#separating-computation-from-storage"
id="toc-separating-computation-from-storage">Separating Computation from
Storage</a></li>
<li><a href="#combining-adders-registers-and-clocks"
id="toc-combining-adders-registers-and-clocks">Combining Adders,
Registers, and Clocks</a></li>
<li><a href="#building-a-counter" id="toc-building-a-counter">Building a
Counter</a></li>
<li><a href="#from-hardware-to-instructions"
id="toc-from-hardware-to-instructions">From Hardware to
Instructions</a></li>
<li><a href="#a-two-instruction-cpu" id="toc-a-two-instruction-cpu">A
Two-Instruction CPU</a></li>
<li><a href="#why-memory-is-required"
id="toc-why-memory-is-required">Why Memory Is Required</a></li>
<li><a href="#program-counter-and-instruction-register"
id="toc-program-counter-and-instruction-register">Program Counter and
Instruction Register</a></li>
<li><a href="#decoding-instructions-with-gates"
id="toc-decoding-instructions-with-gates">Decoding Instructions with
Gates</a></li>
<li><a href="#abstraction-through-repetition"
id="toc-abstraction-through-repetition">Abstraction Through
Repetition</a></li>
<li><a href="#summary-time-makes-programs-possible"
id="toc-summary-time-makes-programs-possible">Summary: Time Makes
Programs Possible</a></li>
<li><a href="#what-comes-next-4" id="toc-what-comes-next-4">What Comes
Next</a></li>
</ul></li>
<li><a
href="#cpu-architecture-and-machine-code-how-programs-actually-run"
id="toc-cpu-architecture-and-machine-code-how-programs-actually-run">CPU
Architecture and Machine Code: How Programs Actually Run</a>
<ul>
<li><a href="#from-fixed-circuits-to-programmable-machines"
id="toc-from-fixed-circuits-to-programmable-machines">From Fixed
Circuits to Programmable Machines</a></li>
<li><a href="#a-simple-but-complete-cpu"
id="toc-a-simple-but-complete-cpu">A Simple but Complete CPU</a></li>
<li><a href="#registers-fast-small-storage"
id="toc-registers-fast-small-storage">Registers: Fast, Small
Storage</a></li>
<li><a href="#memory-where-programs-and-data-live"
id="toc-memory-where-programs-and-data-live">Memory: Where Programs and
Data Live</a></li>
<li><a href="#the-fetchdecodeexecute-cycle"
id="toc-the-fetchdecodeexecute-cycle">The Fetch–Decode–Execute
Cycle</a></li>
<li><a href="#why-hexadecimal-is-used"
id="toc-why-hexadecimal-is-used">Why Hexadecimal Is Used</a></li>
<li><a href="#instruction-formats-and-operands"
id="toc-instruction-formats-and-operands">Instruction Formats and
Operands</a></li>
<li><a href="#control-logic-and-decoding"
id="toc-control-logic-and-decoding">Control Logic and Decoding</a></li>
<li><a href="#sequential-flow-branches-and-loops"
id="toc-sequential-flow-branches-and-loops">Sequential Flow, Branches,
and Loops</a></li>
<li><a href="#example-counting-with-a-loop"
id="toc-example-counting-with-a-loop">Example: Counting with a
Loop</a></li>
<li><a href="#assembly-language-as-a-human-interface"
id="toc-assembly-language-as-a-human-interface">Assembly Language as a
Human Interface</a></li>
<li><a href="#strings-characters-and-memory"
id="toc-strings-characters-and-memory">Strings, Characters, and
Memory</a></li>
<li><a href="#from-python-to-machine-instructions"
id="toc-from-python-to-machine-instructions">From Python to Machine
Instructions</a></li>
<li><a href="#why-this-model-still-matters"
id="toc-why-this-model-still-matters">Why This Model Still
Matters</a></li>
<li><a href="#summary-programs-are-physical-processes"
id="toc-summary-programs-are-physical-processes">Summary: Programs Are
Physical Processes</a></li>
<li><a href="#what-comes-next-5" id="toc-what-comes-next-5">What Comes
Next</a></li>
</ul></li>
<li><a
href="#webassembly-and-emulation-running-real-programs-in-the-browser"
id="toc-webassembly-and-emulation-running-real-programs-in-the-browser">WebAssembly
and Emulation: Running Real Programs in the Browser</a>
<ul>
<li><a href="#from-hardware-to-emulators"
id="toc-from-hardware-to-emulators">From Hardware to Emulators</a></li>
<li><a href="#why-emulation-is-practical-today"
id="toc-why-emulation-is-practical-today">Why Emulation Is Practical
Today</a></li>
<li><a href="#the-cdc6504-emulator" id="toc-the-cdc6504-emulator">The
CDC6504 Emulator</a></li>
<li><a href="#machine-code-inside-software"
id="toc-machine-code-inside-software">Machine Code Inside
Software</a></li>
<li><a href="#from-native-cpus-to-virtual-machines"
id="toc-from-native-cpus-to-virtual-machines">From Native CPUs to
Virtual Machines</a></li>
<li><a href="#what-is-webassembly" id="toc-what-is-webassembly">What Is
WebAssembly?</a></li>
<li><a href="#from-c-to-wasm" id="toc-from-c-to-wasm">From C to
WASM</a></li>
<li><a href="#a-wasm-hello-world" id="toc-a-wasm-hello-world">A WASM
“Hello, World”</a></li>
<li><a href="#wasm-compared-to-traditional-assembly"
id="toc-wasm-compared-to-traditional-assembly">WASM Compared to
Traditional Assembly</a></li>
<li><a href="#loops-functions-and-the-stack"
id="toc-loops-functions-and-the-stack">Loops, Functions, and the
Stack</a></li>
<li><a href="#from-historical-cpus-to-modern-devices"
id="toc-from-historical-cpus-to-modern-devices">From Historical CPUs to
Modern Devices</a></li>
<li><a href="#case-study-apples-cpu-transitions"
id="toc-case-study-apples-cpu-transitions">Case Study: Apple’s CPU
Transitions</a></li>
<li><a href="#why-the-6502-still-matters"
id="toc-why-the-6502-still-matters">Why the 6502 Still Matters</a></li>
<li><a href="#emulation-as-a-learning-tool"
id="toc-emulation-as-a-learning-tool">Emulation as a Learning
Tool</a></li>
<li><a href="#summary-abstraction-without-losing-reality"
id="toc-summary-abstraction-without-losing-reality">Summary: Abstraction
Without Losing Reality</a></li>
<li><a href="#closing-thoughts" id="toc-closing-thoughts">Closing
Thoughts</a></li>
</ul></li>
</ul>
</nav>
<h1
id="from-stonehenge-to-silicon-why-computers-began-with-numbers">From
Stonehenge to Silicon: Why Computers Began with Numbers</h1>
<p></p>
<p>Long before computers were used for communication, entertainment, or
social interaction, they were built for something far more basic:
measuring the world and predicting what would happen next. The earliest
forms of computation were not abstract symbols stored in memory, but
physical structures designed to model natural processes. These devices
helped track the seasons, predict the motion of planets, guide
travelers, and support engineering and trade. In this early period,
computing was inseparable from mathematics and measurement, and numbers
were the primary objects being manipulated.</p>
<p>This focus on numerical calculation remained dominant for most of
computing history. Until the late twentieth century, computers were
rare, expensive, and primarily used for scientific, military, and
industrial purposes. There was no internet, and few people interacted
directly with computing machines. Only later did computers become tools
for information exchange and personal communication. To understand
modern computer architecture, it helps to begin with this earlier world,
where computation meant physically transforming numbers.</p>
<hr />
<h2 id="curved-motion-and-the-need-for-prediction">Curved Motion and the
Need for Prediction</h2>
<p>Much of the natural world moves along curves rather than straight
lines. The arc of a thrown object, the path of the Moon across the sky,
and the orbit of planets all follow curved trajectories. Predicting such
motion is difficult because small errors accumulate over time, and
precise prediction requires repeated calculation. While the human brain
is good at intuitive estimates, accurate forecasting requires systematic
measurement and mathematical modeling.</p>
<p>The practical need to predict motion—for agriculture, navigation, and
astronomy—drove the development of early computational tools. These
tools were not general-purpose machines, but specialized devices that
captured particular physical relationships and made them easier to
reason about. Each device encoded a small piece of mathematics in wood,
stone, or metal.</p>
<hr />
<h2 id="astronomy-and-physical-data-tables">Astronomy and Physical Data
Tables</h2>
<p>From the earliest civilizations, careful observation of the sky
revealed repeating patterns. The positions of the Sun, Moon, and stars
changed in predictable ways over the course of days, months, and years.
Recording these observations allowed calendars to be built, planting
seasons to be planned, and ceremonial events to be scheduled.</p>
<p>Over time, physical structures were constructed to embody these
patterns directly. Instead of writing numbers in tables, builders placed
stones or architectural features so that sunlight or shadows would align
at particular times of year. These structures functioned as durable,
physical data sets that could be read simply by observing the
environment.</p>
<figure>
<img src="images/ch01-stonehenge-sunrise.png"
alt="Seasonal sunrise and sunset alignment at Stonehenge" />
<figcaption aria-hidden="true">Seasonal sunrise and sunset alignment at
Stonehenge</figcaption>
</figure>
<hr />
<h2 id="stonehenge-as-a-measuring-device">Stonehenge as a Measuring
Device</h2>
<p>Stonehenge, constructed in several phases between roughly 5100 and
3600 years ago, illustrates this idea clearly. The arrangement of stones
aligns with sunrise and sunset positions that change throughout the
year. By observing which stones line up with the Sun on a given day,
seasonal transitions can be predicted.</p>
<p>Rather than storing numbers, Stonehenge stored geometric
relationships. Over many generations, observations were effectively
“written” into the landscape. In modern terms, the structure can be
thought of as a calibrated data table, built incrementally through
centuries of refinement.</p>
<p>Similar solar and lunar alignment structures exist across many
cultures, from temples in India to monuments in Central America. All
reflect the same underlying idea: physical construction can encode
numerical patterns from nature.</p>
<blockquote>
<p>Physical computing did not begin with machines. It began with
architecture.</p>
</blockquote>
<hr />
<h2 id="continuous-representations-of-number">Continuous Representations
of Number</h2>
<p>Many early computational devices represented numbers not as symbols,
but as physical positions. A value might correspond to the angle of a
gear, the distance of a slider, or the rotation of a dial. Because these
values could vary smoothly, such systems are called continuous or
analog.</p>
<p>In these devices, mathematical relationships are built into geometry.
Adding distances can perform multiplication when scales are logarithmic.
Rotating disks can solve trigonometric problems by turning angles into
lengths. Instead of executing arithmetic step by step, the device
produces results through physical alignment.</p>
<p>Accurate printed scales are essential for this approach. The
precision of the computation depends directly on the precision of the
markings and the mechanical stability of the device. In effect, the
manufacturing process becomes part of the calculation.</p>
<hr />
<h2 id="gears-as-models-of-the-solar-system">Gears as Models of the
Solar System</h2>
<figure>
<img src="images/ch01-antikythera-gears.png" height="300"
alt="Antikythera mechanism gear layout" />
<figcaption aria-hidden="true">Antikythera mechanism gear
layout</figcaption>
</figure>
<p>The Antikythera mechanism, dating to around 2100 years ago,
represents one of the most sophisticated examples of ancient analog
computing. This device used interlocking gears to model the motion of
the Sun, Moon, and known planets. Some of its gear trains represented
long astronomical cycles spanning decades or even centuries.</p>
<p>Rather than calculating planetary positions numerically, the
mechanism physically enacted the model of the cosmos that astronomers
had developed. Turning a crank advanced time, and the gears moved
accordingly. The computation occurred through mechanical interaction,
not through written arithmetic.</p>
<p>This illustrates a broader principle that remains true today:
computing systems implement models of reality. Whether those models are
built from bronze gears or silicon transistors, the purpose is the
same—to predict behavior by simulating it.</p>
<hr />
<h2 id="practical-analog-computing">Practical Analog Computing</h2>
<figure>
<img src="images/ch01-sliderule-scales.png"
alt="Slide rule logarithmic scales" />
<figcaption aria-hidden="true">Slide rule logarithmic
scales</figcaption>
</figure>
<p>Analog computation did not remain confined to astronomy. Devices such
as slide rules transformed multiplication into addition by using
logarithmic scales. By aligning and sliding rulers, complex calculations
could be performed quickly and reliably.</p>
<p>A particularly practical example is the E6B flight computer, still
used by pilots to compute wind correction angles and ground speed. By
rotating dials and aligning scales, trigonometric relationships are
solved graphically. The device does not know anything about airplanes;
it simply encodes geometric laws that apply to moving vectors.</p>
<p>In each case, the key idea is that physical movement stands in for
mathematical transformation. The device performs computation because its
shape embodies mathematical relationships.</p>
<hr />
<h2 id="discrete-states-and-digital-devices">Discrete States and Digital
Devices</h2>
<p></p>
<p>Not all physical computation is continuous. Some devices operate
using discrete, stable states. A mechanical latch, for example, stays in
one of two positions until enough force is applied to switch it. These
stable configurations allow information to be stored physically.</p>
<p>Clocks provide a clear example of digital behavior in mechanical
systems. A pendulum provides regular timing, while ratchets and gears
count discrete events. When one gear completes a full rotation, it
advances the next gear, producing the familiar progression from seconds
to minutes to hours.</p>
<figure>
<img src="images/ch01-clock-carry.png"
alt="Clock gear train with carry propagation" />
<figcaption aria-hidden="true">Clock gear train with carry
propagation</figcaption>
</figure>
<p>This mechanism also introduces the concept of carry. When one digit
overflows, the next digit is incremented. Mechanical systems must
physically propagate this carry through connected components, a process
that takes time and introduces delays. Similar effects still occur in
electronic circuits, where signals must travel between components.</p>
<hr />
<h2 id="iteration-and-mechanical-automation">Iteration and Mechanical
Automation</h2>
<p></p>
<p>Once addition and counting are possible, repeated operations can be
automated. Multiplication becomes repeated addition, and polynomial
evaluation becomes a structured sequence of arithmetic steps. Adding
motors or cranks allows machines to perform long sequences without human
intervention.</p>
<p>Charles Babbage’s Difference Engine, designed in the nineteenth
century, exploited this idea. It used repeated addition to approximate
complex mathematical functions and generate accurate tables. Although
technology at the time could not easily produce all the required parts,
a complete version built in the late twentieth century demonstrated that
the design itself was sound.</p>
<blockquote>
<p>Architectural ideas often appear long before manufacturing technology
can fully support them.</p>
</blockquote>
<hr />
<h2 id="human-computers-and-early-programming">Human Computers and Early
Programming</h2>
<p></p>
<p>Before electronic machines became common, teams of people performed
large calculations using mechanical aids and strict procedures. These
workers were called computers, and their job was to execute long
sequences of operations reliably.</p>
<p>When electronic computers emerged, much of this procedural knowledge
transferred directly. Programming languages such as FORTRAN reflected
existing mathematical workflows, including loops and accumulation of
results. Writing a program was, in many ways, formalizing what human
computers had already been doing manually.</p>
<p>Thus, programming did not arise as a completely new activity. It
evolved naturally from structured numerical work that had existed for
generations.</p>
<hr />
<h2 id="from-mechanical-to-electronic-switching">From Mechanical to
Electronic Switching</h2>
<p>Mechanical systems are limited by friction, wear, and inertia. As
machines grew faster and more complex, these physical limits became
obstacles. Vacuum tubes offered a way to perform switching
electronically, without moving parts.</p>
<figure>
<img src="images/ch01-vacuum-tube-switch.png"
alt="Vacuum tubes in the Colossus" />
<figcaption aria-hidden="true">Vacuum tubes in the Colossus</figcaption>
</figure>
<p>Although tubes are inherently analog devices, additional circuitry
allowed them to behave digitally by latching into stable high or low
voltage states. Machines such as Colossus used thousands of tubes to
perform computations far faster than electromechanical systems could
achieve.</p>
<p>Heat and power consumption remained serious challenges, but
electronic switching marked a fundamental shift. Computation was no
longer constrained by mechanical motion.</p>
<hr />
<h2 id="why-architecture-begins-with-numbers">Why Architecture Begins
with Numbers</h2>
<p>Across thousands of years, computational devices were developed to
support astronomy, navigation, engineering, and accounting. These
systems manipulated numbers that represented physical quantities: time,
distance, angle, and mass. The architectural ideas that
emerged—counting, carrying, iteration, state, and switching—remain
central to computer design today.</p>
<p>Only later did computers become tools for processing text, images,
and communication. Those capabilities were built on top of numerical
foundations that had already been refined through centuries of physical
computing devices.</p>
<p>Understanding this origin helps explain why modern processors still
devote most of their structure to arithmetic and control of repeated
operations.</p>
<hr />
<h2 id="what-comes-next">What Comes Next</h2>
<p>The transition from mechanical and electronic switching to
solid-state devices transformed both the speed and scale of computation.
In the next chapter, attention turns to how transistors replaced tubes
and gears, and how tiny electrical switches became the building blocks
of modern computer systems.</p>
<h1
id="from-tubes-to-transistors-how-solid-state-electronics-changed-everything">From
Tubes to Transistors: How Solid-State Electronics Changed
Everything</h1>
<p>The earliest electronic computers replaced mechanical motion with
electrical switching, but they still relied on bulky, fragile components
that consumed large amounts of power. These components, called vacuum
tubes or valves, made it possible to build fully electronic machines,
yet they also imposed severe limits on speed, size, and reliability. The
transition from tubes to transistors did far more than improve existing
designs—it reshaped what computers could be and made modern computing
possible.</p>
<p>This chapter follows that transition. It begins with electronic
amplification, moves through the physics of semiconductors, and ends
with logic gates built from complementary transistor pairs. Along the
way, the story shifts from analog behavior to digital abstraction,
laying the groundwork for everything that follows in computer
architecture.</p>
<hr />
<h2 id="electronic-switching-with-vacuum-tubes">Electronic Switching
with Vacuum Tubes</h2>
<figure>
<img src="images/ch02-vacuum-tube-triode.png"
alt="Vacuum tube triode structure and electron flow" />
<figcaption aria-hidden="true">Vacuum tube triode structure and electron
flow</figcaption>
</figure>
<p>Vacuum tubes were the first practical electronic devices capable of
amplifying and switching signals. In the United Kingdom they were
commonly called valves, a name that reflects their function: controlling
the flow of electrons much like a valve controls the flow of water.
Invented in the early twentieth century, tubes were originally developed
to amplify weak analog signals, especially for long‑distance telephone
communication, where signals had to be boosted every few miles.</p>
<p>By adjusting the design of a tube, engineers could make it behave
more like a switch than an amplifier. When the input voltage crossed a
threshold, the output would move rapidly from low to high voltage. This
made it possible to represent logical states using electrical levels:
low voltage for zero and high voltage for one.</p>
<p>Electronic switching was dramatically faster than mechanical relays,
and it had no moving parts. However, tubes required internal heaters to
release electrons from the cathode, which meant high power consumption
and significant heat. They were also physically large, expensive to
manufacture, and prone to failure over time.</p>
<p>Despite these limitations, tubes enabled the first generation of
fully electronic computers, including machines such as Colossus, which
performed calculations at speeds that mechanical systems could not
approach.</p>
<hr />
<h2 id="the-limits-of-tube-based-computing">The Limits of Tube-Based
Computing</h2>
<p>As computers grew larger and more complex, the drawbacks of vacuum
tubes became increasingly serious. Thousands of tubes meant thousands of
potential failure points. Cooling systems became massive engineering
projects in their own right. Electrical power consumption limited how
dense and how fast circuits could become.</p>
<p>The fundamental problem was not the logic itself, but the physical
mechanism used to implement it. A better switching device was needed—one
that did not rely on heating metal structures or maintaining vacuum
environments.</p>
<p>That solution emerged from solid-state physics.</p>
<hr />
<h2 id="semiconductors-and-controlled-conductivity">Semiconductors and
Controlled Conductivity</h2>
<figure>
<img src="images/ch02-semiconductor-doping.png"
alt="Crystal lattice with P-type and N-type regions" />
<figcaption aria-hidden="true">Crystal lattice with P-type and N-type
regions</figcaption>
</figure>
<p>Most materials fall into one of two categories when it comes to
electricity: conductors, which allow current to flow easily, and
insulators, which resist current strongly. Semiconductors occupy the
middle ground. Under the right conditions, they can be made to conduct
or block current in controlled ways.</p>
<p>Silicon, one of the most common elements in the Earth’s crust,
becomes useful for electronics when small amounts of other elements are
added to it. This process, called doping, changes how electrons move
through the crystal lattice. Regions with extra electrons are called
N-type, while regions missing electrons are called P-type.</p>
<p>When these regions meet, electrical behavior emerges that can be
precisely controlled by external voltages. At the boundaries between
P-type and N-type material, electric fields form that regulate the
movement of charge carriers. These microscopic interactions make it
possible to build devices that switch and amplify signals without moving
parts or heaters.</p>
<hr />
<h2 id="transistors-as-electronic-switches">Transistors as Electronic
Switches</h2>
<figure>
<img src="images/ch02-transistor-evolution.png"
alt="First point-contact transistor and modern MOSFET comparison" />
<figcaption aria-hidden="true">First point-contact transistor and modern
MOSFET comparison</figcaption>
</figure>
<p>The transistor was developed as a solid-state replacement for the
triode vacuum tube. Like tubes, transistors could amplify signals and
could also be designed to behave as switches. But unlike tubes,
transistors were small, required little power, and generated far less
heat.</p>
<p>Early transistors were built using bipolar junction designs, where
current flowing through one region controlled current in another. Later
designs used metal-oxide-semiconductor structures, where an electric
field at a control terminal called the gate regulated conduction through
a channel of semiconductor material.</p>
<p>In both cases, the key property was the same: a small input signal
could reliably control a larger output current. This allowed transistors
to serve as building blocks for both analog amplifiers and digital logic
circuits.</p>
<p>As manufacturing techniques improved, transistors became smaller,
faster, and cheaper. What began as laboratory prototypes soon became
mass-produced components found in radios, calculators, and eventually
computers.</p>
<hr />
<h2 id="analog-amplifiers-and-digital-switching">Analog Amplifiers and
Digital Switching</h2>
<p>Transistors did not immediately replace tubes in all applications.
Audio amplification, for example, has long relied on both technologies,
and some musicians and audio engineers still prefer tube-based
amplifiers for their characteristic distortion patterns.</p>
<p>However, digital logic places very different demands on electronic
components. In digital systems, what matters most is not the precise
shape of a waveform, but whether a signal is interpreted as high or low.
Designs that move quickly and decisively between these two states are
far more useful than those that reproduce subtle analog variations.</p>
<p>To support digital operation, transistor designs were tuned to behave
like fast, reliable switches rather than smooth amplifiers. Circuits
were engineered so that small deviations in input voltage would be
corrected at the output, restoring clean logical values. This behavior,
called signal regeneration, is essential for building large digital
systems where noise and small errors would otherwise accumulate.</p>
<hr />
<h2 id="nmos-pmos-and-complementary-pairs">NMOS, PMOS, and Complementary
Pairs</h2>
<figure>
<img src="images/ch02-nmos-pmos.png"
alt="NMOS and PMOS transistor symbols and conduction paths" />
<figcaption aria-hidden="true">NMOS and PMOS transistor symbols and
conduction paths</figcaption>
</figure>
<p>Different transistor structures conduct under different conditions.
NMOS transistors conduct when their gate voltage is high, while PMOS
transistors conduct when their gate voltage is low. Each type has
advantages and disadvantages when used alone.</p>
<p>Early integrated circuits often used only one type of transistor,
resulting in designs that consumed power even when not switching and
generated significant heat. As chip densities increased, this became a
serious limitation.</p>
<p>The solution was to pair NMOS and PMOS transistors in complementary
configurations. In such arrangements, when one transistor is on, the
other is off. This drastically reduces static power consumption and
improves switching behavior. The resulting technology is called CMOS,
short for Complementary Metal‑Oxide‑Semiconductor.</p>
<p>Once CMOS manufacturing became practical in the late twentieth
century, it transformed computer design. Entire processors could be
placed on single chips, power requirements dropped dramatically, and
circuit densities increased by orders of magnitude.</p>
<p>From this point forward, nearly all digital logic in mainstream
computing has been built using CMOS technology.</p>
<hr />
<h2 id="from-transistors-to-logic-gates">From Transistors to Logic
Gates</h2>
<figure>
<img src="images/ch02-cmos-not-gate.png"
alt="CMOS inverter layout showing pull-up and pull-down networks" />
<figcaption aria-hidden="true">CMOS inverter layout showing pull-up and
pull-down networks</figcaption>
</figure>
<p>While circuits are built from transistors, designers rarely think in
terms of individual switching devices when creating complex systems.
Instead, transistors are grouped into higher-level structures called
logic gates. A gate accepts one or more binary inputs and produces a
binary output according to a logical rule.</p>
<p>The simplest gate is the inverter, or NOT gate, which produces the
opposite of its input. In CMOS, an inverter is built from one NMOS
transistor and one PMOS transistor arranged so that exactly one conducts
at any time. When the input is low, the PMOS transistor pulls the output
high. When the input is high, the NMOS transistor pulls the output
low.</p>
<p>This complementary behavior provides fast switching and low power
usage, making it ideal for large-scale digital systems.</p>
<hr />
<h2 id="building-complexity-from-simple-gates">Building Complexity from
Simple Gates</h2>
<p>Once reliable gates are available, more complex logical functions can
be constructed by combining them. Gates such as AND, OR, and exclusive
OR implement familiar logical operations. Other gates, such as NAND and
NOR, are especially important because entire digital systems can be
built using only one of these gate types.</p>
<figure>
<img src="images/ch02-nand-nor-layout.png"
alt="NAND and NOR gate transistor-level structures" />
<figcaption aria-hidden="true">NAND and NOR gate transistor-level
structures</figcaption>
</figure>
<p>Designing digital circuits at the gate level allows engineers to
reason about behavior using binary logic instead of voltage levels and
transistor physics. This abstraction makes it possible to build systems
containing billions of transistors without managing each device
individually.</p>
<p>From half adders and full adders to registers and processors, nearly
every component in a computer can be described as an arrangement of
logic gates operating in synchronized patterns.</p>
<hr />
<h2 id="why-cmos-made-modern-computers-possible">Why CMOS Made Modern
Computers Possible</h2>
<p>Before CMOS, logic circuits were relatively slow, generated large
amounts of heat, and could not be densely packed. Computers filled rooms
and required elaborate cooling and power infrastructure. With the rise
of CMOS, these constraints relaxed dramatically.</p>
<p>As transistor sizes shrank and manufacturing improved, entire central
processing units moved from cabinets to circuit boards, and then onto
single integrated chips. Power efficiency improved, clock speeds
increased, and reliability soared.</p>
<p>This shift did not change the logical structure of computation, but
it changed the physical feasibility of building large and fast systems.
The same architectural ideas—state, control, and iteration—could now be
implemented at scales that earlier engineers could only imagine.</p>
<hr />
<h2 id="summary-solid-state-foundations-of-digital-logic">Summary:
Solid-State Foundations of Digital Logic</h2>
<p>The move from vacuum tubes to transistors replaced fragile,
power-hungry components with small, efficient solid-state devices.
Advances in semiconductor physics and manufacturing enabled reliable
electronic switching without mechanical motion or heated filaments.</p>
<p>Complementary transistor designs made CMOS the dominant technology
for digital logic, allowing dense, low-power circuits to become
practical. By grouping transistors into logic gates, designers gained an
abstraction that supports the construction of complex systems without
constant reference to underlying physics.</p>
<p>With these foundations in place, attention can now shift from
individual devices to how large collections of gates are organized into
functional computing units.</p>
<hr />
<h2 id="what-comes-next-1">What Comes Next</h2>
<p>With solid-state logic established, the next step is to explore how
gates are combined into arithmetic circuits and memory structures. The
following chapter examines how simple logical components are assembled
into adders, registers, and the building blocks of processors.</p>
<h1
id="very-large-scale-integration-from-individual-devices-to-entire-chips">Very
Large Scale Integration: From Individual Devices to Entire Chips</h1>
<p>The invention of the transistor made electronic switching reliable
and efficient, but early circuits still consisted of individual
components wired together on circuit boards. Each transistor, resistor,
and capacitor had to be manufactured separately and then connected by
hand or by automated assembly. While this approach worked for small
systems, it quickly became impractical as circuits grew more
complex.</p>
<p>Very Large Scale Integration (VLSI) describes the set of technologies
that made it possible to place enormous numbers of transistors onto a
single piece of silicon. Instead of assembling computers from individual
components, entire processors could be manufactured as unified physical
systems. This shift did not merely improve performance; it fundamentally
changed how computers were designed, built, and scaled.</p>
<hr />
<h2 id="from-discrete-components-to-integrated-circuits">From Discrete
Components to Integrated Circuits</h2>
<p>Early transistor-based electronics were constructed much like
mechanical systems: individual parts were mounted on boards and
connected with wires or metal traces. Even when integrated circuits
first appeared, they typically contained only a handful of transistors
or logic gates.</p>
<p>These early chips reduced size and improved reliability, but they did
not yet enable full processors to be placed on a single device. Complex
systems still required many chips working together, with signals
traveling across boards and connectors. Speed was limited by how fast
electrical signals could move between packages, and cost increased with
every additional component.</p>
<p>The next major leap required changing not just circuit design, but
manufacturing itself.</p>
<hr />
<h2 id="planar-transistors-and-two-dimensional-manufacturing">Planar
Transistors and Two-Dimensional Manufacturing</h2>
<figure>
<img src="images/ch03-planar-transistor.png"
alt="Planar transistor cross-section and surface fabrication" />
<figcaption aria-hidden="true">Planar transistor cross-section and
surface fabrication</figcaption>
</figure>
<p>Before the 1960s, transistors were often built as small
three-dimensional structures that had to be individually assembled and
wired. This made large-scale manufacturing slow, expensive, and
difficult to automate.</p>
<p>The planar transistor process, developed at Fairchild Semiconductor,
transformed transistor fabrication into a two-dimensional surface
process. Instead of assembling parts, chemical and photographic
techniques were used to shape transistor structures directly on the
surface of silicon wafers.</p>
<p>By depositing materials in layers and selectively removing regions
using masks, entire arrays of transistors could be created
simultaneously. This approach allowed thousands, and eventually
billions, of devices to be produced in a single manufacturing run.</p>
<p>Planar fabrication made integration scalable.</p>
<hr />
<h2 id="wafers-masks-and-photolithography">Wafers, Masks, and
Photolithography</h2>
<figure>
<img src="images/ch03-silicon-wafer-dies.png"
alt="Silicon wafer with multiple die patterns" />
<figcaption aria-hidden="true">Silicon wafer with multiple die
patterns</figcaption>
</figure>
<p>Modern chip manufacturing begins with thin circular slices of silicon
called wafers. Each wafer contains many identical copies of a chip
design, arranged in a grid pattern. After fabrication, the wafer is cut
into individual chips, each of which becomes a processor, memory device,
or controller.</p>
<p>Patterns are transferred onto wafers using photolithography. In this
process, light-sensitive chemicals are exposed through precisely
designed masks. Each mask defines where material will be added, removed,
or altered during that step of fabrication.</p>
<p>Multiple masks are used in sequence to build up complex
three-dimensional structures from stacked layers. Although the
manufacturing process is layered, the design itself is typically
described as a two-dimensional layout, where different materials occupy
specific regions and intersections.</p>
<p>This manufacturing model is what allows enormous numbers of
transistors to be created with extremely high consistency.</p>
<hr />
<h2 id="growth-of-transistor-density-over-time">Growth of Transistor
Density Over Time</h2>
<figure>
<img src="images/ch03-transistor-growth.png"
alt="Historical growth of transistor counts on processors" />
<figcaption aria-hidden="true">Historical growth of transistor counts on
processors</figcaption>
</figure>
<p>Once planar manufacturing was established, transistor counts began to
grow rapidly. The Intel 4004 microprocessor, released in 1971, contained
approximately 2,300 transistors and was the first commercially available
single-chip general-purpose processor.</p>
<p>Over the following decades, transistor counts increased by orders of
magnitude. By the early 2000s, chips contained billions of transistors.
Modern graphics processors and specialized accelerators now contain tens
or even hundreds of billions of devices, and experimental wafer-scale
systems integrate trillions of transistors across entire wafers.</p>
<p>This steady growth is often associated with Moore’s Law, an
observation that transistor density tends to double over fixed time
intervals. While physical limits now constrain how small transistors can
become, integration remains the defining feature of modern computing
hardware.</p>
<hr />
<h2 id="designing-with-layout-not-wires">Designing with Layout, Not
Wires</h2>
<p>As transistor counts increased, manual wiring became impossible.
Instead, designers describe circuits using layout patterns that specify
where materials will be placed on the wafer.</p>
<p>VLSI layout tools allow engineers to design using layers that
represent different physical materials:</p>
<ul>
<li>metal for conductors<br />
</li>
<li>doped silicon regions for transistor channels<br />
</li>
<li>polysilicon for control gates</li>
</ul>
<p>When certain layers cross, transistors are formed automatically by
the manufacturing process. Vertical connections between layers are
created using structures called vias, which allow signals to move
between wiring layers.</p>
<figure>
<img src="images/ch03-vlsi-layers.png"
alt="Layered VLSI layout showing metal, diffusion, and polysilicon" />
<figcaption aria-hidden="true">Layered VLSI layout showing metal,
diffusion, and polysilicon</figcaption>
</figure>
<p>Rather than drawing individual transistors explicitly, designers
arrange geometric regions whose interactions produce the desired
electrical behavior. Logical structure emerges from physical
geometry.</p>
<hr />
<h2 id="from-layout-to-logic-gates">From Layout to Logic Gates</h2>
<p>Just as transistors are grouped into logic gates in circuit design,
layout patterns are arranged to produce gate behavior at the physical
level.</p>
<p>A CMOS inverter, for example, is created by arranging one PMOS and
one NMOS transistor so that exactly one conducts for any input value.
When input voltage is low, the PMOS pulls the output high. When input
voltage is high, the NMOS pulls the output low.</p>
<figure>
<img src="images/ch03-vlsi-not-layout.png"
alt="CMOS inverter layout in VLSI form" />
<figcaption aria-hidden="true">CMOS inverter layout in VLSI
form</figcaption>
</figure>
<p>More complex gates such as NAND and NOR are built by combining
multiple transistors in series and parallel arrangements. The same
logical relationships studied in gate diagrams appear again in physical
form as geometric structures on silicon.</p>
<figure>
<img src="images/ch03-vlsi-nand-layout.png"
alt="CMOS NAND gate VLSI layout" />
<figcaption aria-hidden="true">CMOS NAND gate VLSI layout</figcaption>
</figure>
<p>This repetition of structure across abstraction levels is one of the
defining characteristics of computer architecture: logical design
mirrors physical implementation.</p>
<hr />
<h2 id="design-tools-and-educational-models">Design Tools and
Educational Models</h2>
<p>Professional VLSI design requires sophisticated tools that model
electrical behavior, timing, power consumption, and manufacturing
constraints. One of the most widely used educational tools for learning
layout is the open-source Magic VLSI system, originally developed at
Berkeley in the early 1980s and still widely used in academic
instruction.</p>
<p>Educational layout environments simplify many aspects of real
fabrication. They may allow layer overlaps or ignore detailed spacing
rules that would be impossible to manufacture reliably. These
simplifications make it easier to understand core ideas, even if the
resulting layouts would not be suitable for commercial fabrication.</p>
<p>Simplified emulators and browser-based tools used for instruction
focus on conceptual correctness rather than full physical accuracy. They
illustrate how geometry produces logical behavior without modeling every
manufacturing constraint.</p>
<hr />
<h2 id="why-integration-changes-architecture">Why Integration Changes
Architecture</h2>
<p>When entire systems fit onto single chips, communication between
components becomes far faster and more reliable. Signals no longer need
to traverse long board traces or pass through connectors between
packages. Memory, arithmetic units, and control logic can be tightly
integrated and optimized together.</p>
<p>This physical proximity enables architectural features that would be
impractical in discrete-component systems, such as deep pipelines, large
on-chip caches, and complex parallel execution units.</p>
<p>Integration also shifts economic considerations. Instead of
assembling systems from parts, the primary cost becomes chip design and
fabrication, while replication becomes inexpensive once manufacturing is
established.</p>
<p>As a result, architectural decisions increasingly reflect trade-offs
between silicon area, power consumption, and performance rather than
assembly complexity.</p>
<hr />
<h2 id="summary-manufacturing-enables-abstraction">Summary:
Manufacturing Enables Abstraction</h2>
<p>Very Large Scale Integration made it possible to place entire
computing systems onto single pieces of silicon. Planar fabrication,
photolithography, and layered manufacturing transformed transistor
construction into a massively parallel industrial process.</p>
<p>Layout-based design replaced manual wiring, and logical structures
emerged from physical geometry. Transistors became gates, gates became
arithmetic units, and complete processors became manufacturable as
single devices.</p>
<p>With integration established, the focus of computer architecture can
now shift away from manufacturing techniques and toward how logic itself
is organized to perform computation.</p>
<hr />
<h2 id="what-comes-next-2">What Comes Next</h2>
<p>With logic gates available as reliable building blocks, the next step
is to examine how arithmetic operations and memory storage are
implemented using combinations of gates. The following chapter explores
how numbers are added, stored, and manipulated using purely digital
logic structures.</p>
<h1 id="digital-logic-building-computation-from-gates">Digital Logic:
Building Computation from Gates</h1>
<p>With reliable transistors and scalable manufacturing in place,
attention can shift from how devices are built to how computation itself
is organized. Digital logic provides the abstraction that allows
electrical behavior to be treated as mathematical structure. Voltages
become symbols, wires become signals, and circuits become logical
systems that can be reasoned about using rules rather than physical
measurements.</p>
<p>This chapter introduces the logical building blocks of computers:
gates, adders, and storage elements. These components form the
foundation of processors and memory systems.</p>
<hr />
<h2 id="computers-as-interconnected-components">Computers as
Interconnected Components</h2>
<p>Inside a computer, information moves along wires that carry
electrical signals. Each signal is interpreted as either a logical zero
or one. Groups of wires connect major subsystems such as the central
processing unit, memory, and input/output devices.</p>
<figure>
<img src="images/ch04-generic-computer.png"
alt="Generic computer system showing CPU, memory, and I/O" />
<figcaption aria-hidden="true">Generic computer system showing CPU,
memory, and I/O</figcaption>
</figure>
<p>Although software describes computation in abstract terms, every
operation ultimately becomes patterns of electrical signals traveling
between physical components. Digital logic provides the framework that
connects these physical movements to symbolic meaning.</p>
<hr />
<h2 id="what-the-cpu-does">What the CPU Does</h2>
<p>The central processing unit (CPU) executes programs by repeatedly
performing a simple cycle: fetch an instruction, interpret it, and
perform the required operation. The CPU is not intelligent in the human
sense. It does not understand goals or meaning. Instead, it follows
mechanical rules at very high speed, executing billions of operations
per second in modern systems.</p>
<p>Programs written in high-level languages are translated into machine
instructions that the CPU can execute directly. Each instruction
specifies small operations such as moving data, performing arithmetic,
or testing conditions.</p>
<p>At the hardware level, these operations are implemented entirely
using combinations of logic gates and storage elements.</p>
<hr />
<h2 id="logic-gates-as-building-blocks">Logic Gates as Building
Blocks</h2>
<p>Logic gates accept one or more binary inputs and produce a binary
output. Each gate implements a simple logical rule.</p>
<p>The most common gates include:</p>
<ul>
<li><strong>NOT</strong>, which inverts a signal<br />
</li>
<li><strong>AND</strong>, which produces one only if all inputs are
one<br />
</li>
<li><strong>OR</strong>, which produces one if any input is one<br />
</li>
<li><strong>XOR</strong>, which produces one if inputs differ</li>
</ul>
<figure>
<img src="images/ch04-basic-gates.png"
alt="Truth tables and symbols for basic logic gates" />
<figcaption aria-hidden="true">Truth tables and symbols for basic logic
gates</figcaption>
</figure>
<p>Although these operations are simple, they are sufficient to
construct any digital computation when combined appropriately.</p>
<hr />
<h2 id="representing-numbers-in-binary">Representing Numbers in
Binary</h2>
<p>To perform arithmetic using logic gates, numbers must be represented
using electrical signals. Humans normally use base‑10 representation,
where each digit represents a power of ten. Digital systems instead use
base‑2 representation, where each digit represents a power of two.</p>
<p>For example:</p>
<ul>
<li>The base‑10 number 6 is written as <strong>110₂</strong> in
binary.<br />
</li>
<li>The base‑10 number 7 is written as <strong>111₂</strong>.</li>
</ul>
<p>Each bit position corresponds to a weight:</p>
<ul>
<li>leftmost bit → 4<br />
</li>
<li>middle bit → 2<br />
</li>
<li>rightmost bit → 1</li>
</ul>
<p>Binary representation allows numerical values to be manipulated using
simple logical operations on individual bits.</p>
<figure>
<img src="images/ch04-binary-place-values.png"
alt="Binary place values for three-bit numbers" />
<figcaption aria-hidden="true">Binary place values for three-bit
numbers</figcaption>
</figure>
<hr />
<h2 id="adding-numbers-with-gates-half-adders">Adding Numbers with
Gates: Half Adders</h2>
<p>The simplest arithmetic operation is addition. When adding two single
bits, there are four possible input combinations. The result must
produce both a sum bit and a carry bit.</p>
<p>A <strong>half adder</strong> is a circuit that adds two bits and
produces:</p>
<ul>
<li>a <strong>sum</strong> output<br />
</li>
<li>a <strong>carry</strong> output</li>
</ul>
<p>The sum output is produced by an XOR gate, while the carry output is
produced by an AND gate.</p>
<figure>
<img src="images/ch04-half-adder.png"
alt="Half adder logic diagram and truth table" />
<figcaption aria-hidden="true">Half adder logic diagram and truth
table</figcaption>
</figure>
<p>This circuit performs correct binary addition for single-bit
values.</p>
<hr />
<h2 id="full-adders-and-multi-bit-addition">Full Adders and Multi-Bit
Addition</h2>
<p>When adding multi-bit numbers, each bit position must also consider a
carry value from the previous position. A <strong>full adder</strong>
extends the half adder by adding three inputs:</p>
<ul>
<li>bit A<br />
</li>
<li>bit B<br />
</li>
<li>carry‑in</li>
</ul>
<p>It produces:</p>
<ul>
<li>a sum bit<br />
</li>
<li>a carry‑out bit</li>
</ul>
<p>By chaining full adders together, multi-bit addition can be
performed. Each stage passes its carry output to the next stage,
allowing numbers of arbitrary length to be added.</p>
<figure>
<img src="images/ch04-full-adder-chain.png"
alt="Chained full adders forming a multi-bit adder" />
<figcaption aria-hidden="true">Chained full adders forming a multi-bit
adder</figcaption>
</figure>
<p>In practice, processors use more sophisticated adder designs to
improve speed, but the fundamental principle remains the same.</p>
<hr />
<h2 id="storing-data-with-feedback">Storing Data with Feedback</h2>
<p>Computation requires not only processing data but also remembering
it. Storage is implemented using circuits that maintain state over
time.</p>
<p>The simplest storage element uses <strong>feedback</strong>, where
part of the output is fed back into the input of the circuit. This
allows a value to persist even when the original input signal is
removed.</p>
<p>An example is the <strong>set-reset (SR) latch</strong>, built from
two cross‑connected NOR gates. Depending on the control inputs, the
latch can store either a zero or a one.</p>
<figure>
<img src="images/ch04-sr-latch.png"
alt="SR latch built from cross-coupled NOR gates" />
<figcaption aria-hidden="true">SR latch built from cross-coupled NOR
gates</figcaption>
</figure>
<p>Feedback loops introduce a new behavior: the circuit’s output depends
not only on current inputs, but also on past states.</p>
<hr />
<h2 id="clocked-storage-gated-d-latches">Clocked Storage: Gated D
Latches</h2>
<p>While simple latches can store data, processors require more
controlled storage that changes only at specific times. This is achieved
by introducing a clock signal.</p>
<p>A <strong>gated D latch</strong> has:</p>
<ul>
<li>a data input (D)<br />
</li>
<li>a clock or control input (C)</li>
</ul>
<p>When the clock is active, the latch copies the data input into its
internal state. When the clock is inactive, the stored value is held
constant regardless of changes to the input.</p>
<figure>
<img src="images/ch04-gated-d-latch.png"
alt="Gated D latch timing and structure" />
<figcaption aria-hidden="true">Gated D latch timing and
structure</figcaption>
</figure>
<p>This behavior allows many storage elements to update in synchronized
steps, forming the basis of registers and memory systems.</p>
<hr />
<h2 id="registers-from-multiple-latches">Registers from Multiple
Latches</h2>
<p>By grouping multiple gated D latches together, multi-bit storage
units can be created. For example, three latches can store a three-bit
number. Larger registers store entire machine words, allowing processors
to hold intermediate values during computation.</p>
<p>Registers provide fast, temporary storage that supports arithmetic
operations, branching decisions, and data movement within the CPU.</p>
<figure>
<img src="images/ch04-register.png"
alt="Three-bit register built from gated D latches" />
<figcaption aria-hidden="true">Three-bit register built from gated D
latches</figcaption>
</figure>
<p>These structures form the immediate working memory of the
processor.</p>
<hr />
<h2 id="tools-for-exploring-digital-logic">Tools for Exploring Digital
Logic</h2>
<p>Modern educational tools allow digital circuits to be built and
tested interactively. Gate-level simulators can display signal flow,
timing, and logical behavior, making it easier to understand how complex
systems emerge from simple components.</p>
<p>Some tools support layout-level construction, while others focus on
abstract gate connectivity. Both approaches reinforce the relationship
between physical hardware and logical structure.</p>
<p>By experimenting with gates, adders, and latches, it becomes clear
that computation arises not from individual devices but from organized
patterns of interaction.</p>
<hr />
<h2 id="summary-from-gates-to-computation">Summary: From Gates to
Computation</h2>
<p>Digital logic transforms electrical behavior into mathematical
structure. Logic gates implement simple rules, adders perform
arithmetic, and latches store information over time.</p>
<p>By combining these components, complex machines can be built that
execute programs, manipulate data, and respond to inputs. The physical
realities of electronics remain present, but abstraction allows
designers to reason about systems in logical terms.</p>
<p>With digital logic in place, the final step toward building a
processor is introducing coordinated timing and instruction
sequencing.</p>
<hr />
<h2 id="what-comes-next-3">What Comes Next</h2>
<p>The next chapter introduces clocked circuits and control logic. These
mechanisms coordinate when data moves and when operations occur,
allowing entire programs to be executed step by step inside the
processor.</p>
<h1 id="clocked-circuits-coordinating-computation-over-time">Clocked
Circuits: Coordinating Computation Over Time</h1>
<p>Combinational logic circuits, such as adders and logic gates, produce
outputs that depend only on their current inputs. However, physical
circuits do not update instantaneously. Signals require time to
propagate through transistors and wires before stabilizing at their
final values. As circuits grow larger and more complex, this delay
becomes increasingly important.</p>
<p>To build reliable systems, digital computers introduce a coordinating
signal called a <strong>clock</strong>. The clock defines discrete
moments when values are allowed to change, allowing computation to
proceed in synchronized steps rather than continuously drifting through
intermediate states.</p>
<hr />
<h2 id="propagation-delay-and-circuit-length">Propagation Delay and
Circuit Length</h2>
<p>When an input changes, it takes time for the effects of that change
to travel through the circuit. Each transistor introduces a small delay,
and long paths through many devices accumulate larger delays.</p>
<p>For example:</p>
<ul>
<li>a simple gate settles quickly<br />
</li>
<li>a multi-bit adder takes longer<br />
</li>
<li>a multiplier or complex control path may take much longer</li>
</ul>
<p>If outputs are observed before signals have fully settled, incorrect
values may be captured. Circuit designers therefore identify the
<strong>slowest path</strong> through a system, known as the critical
path, and ensure that enough time passes before results are used.</p>
<figure>
<img src="images/ch05-propagation-delay.png"
alt="Signal propagation through chained logic elements" />
<figcaption aria-hidden="true">Signal propagation through chained logic
elements</figcaption>
</figure>
<hr />
<h2 id="clock-rate-and-system-timing">Clock Rate and System Timing</h2>
<p>The clock rate of a processor specifies how often values are allowed
to be updated. Each clock cycle provides time for signals to propagate
and stabilize before being stored.</p>
<p>The maximum clock frequency is limited by:</p>
<ul>
<li>the physical length of signal paths<br />
</li>
<li>the number of transistors in the longest logical path<br />
</li>
<li>the switching speed of the transistors<br />
</li>
<li>the size of the chip itself</li>
</ul>
<p>Once the slowest element of the arithmetic and logic unit (ALU) is
identified, the clock must be slow enough to accommodate that delay.
Faster clocks allow more operations per second, but only if circuits can
reliably settle within each cycle.</p>
<figure>
<img src="images/ch05-clock-waveform.png"
alt="Clock waveform showing discrete sampling points" />
<figcaption aria-hidden="true">Clock waveform showing discrete sampling
points</figcaption>
</figure>
<hr />
<h2 id="separating-computation-from-storage">Separating Computation from
Storage</h2>
<p>To coordinate circuits, designers separate systems into two major
categories:</p>
<ul>
<li><strong>combinational circuits</strong>, which compute
continuously<br />
</li>
<li><strong>clocked storage elements</strong>, which update only on
clock events</li>
</ul>
<p>Adders and logic gates belong to the first category. Latches and
registers belong to the second.</p>
<p>During most of the clock cycle, combinational circuits compute based
on stable stored inputs. At the clock edge, new values are captured into
storage elements, and the next cycle of computation begins.</p>
<p>This separation prevents unstable intermediate values from
propagating into stored state.</p>
<hr />
<h2 id="combining-adders-registers-and-clocks">Combining Adders,
Registers, and Clocks</h2>
<p>Consider a simple system that adds two numbers and stores the
result:</p>
<ul>
<li>a register holds the current value<br />
</li>
<li>an adder computes a new value<br />
</li>
<li>a clock controls when the register updates</li>
</ul>
<p>While the clock is low, the register holds its value and the adder
continuously computes the sum based on that value. When the clock goes
high, the computed sum is captured into the register. The next cycle
begins with a new stable value.</p>
<figure>
<img src="images/ch05-adder-register.png"
alt="Adder feeding a register under clock control" />
<figcaption aria-hidden="true">Adder feeding a register under clock
control</figcaption>
</figure>
<p>This structure is repeated throughout processors to create
step-by-step execution.</p>
<hr />
<h2 id="building-a-counter">Building a Counter</h2>
<p>By feeding the output of a register back into one input of an adder
and fixing the other input to the value one, a counting circuit can be
created.</p>
<p>Each clock cycle:</p>
<ol type="1">
<li>the adder computes current value plus one<br />
</li>
<li>the register stores the new value<br />
</li>
<li>the process repeats</li>
</ol>
<p>After reaching the maximum representable value, the counter overflows
and wraps back to zero.</p>
<figure>
<img src="images/ch05-counter.png"
alt="Counter built from adder and register" />
<figcaption aria-hidden="true">Counter built from adder and
register</figcaption>
</figure>
<p>This simple structure forms the basis of timers, program counters,
and many sequencing mechanisms inside computers.</p>
<hr />
<h2 id="from-hardware-to-instructions">From Hardware to
Instructions</h2>
<p>So far, all behavior has been determined by fixed wiring. To build
programmable machines, behavior must be controlled by
<strong>instructions</strong> rather than physical switches.</p>
<p>An instruction is a pattern of bits that specifies which operations
should occur during a clock cycle. Instead of permanently connecting
wires to force an action, instruction bits enable or disable parts of
the circuit dynamically.</p>
<p>This is achieved through <strong>control logic</strong> that
interprets instruction bits and routes signals accordingly.</p>
<hr />
<h2 id="a-two-instruction-cpu">A Two-Instruction CPU</h2>
<p>A minimal processor can be built using:</p>
<ul>
<li>a small register<br />
</li>
<li>an adder<br />
</li>
<li>control gates<br />
</li>
<li>a clock</li>
</ul>
<p>Suppose the machine supports two instructions:</p>
<ul>
<li><strong>0</strong> — clear the register<br />
</li>
<li><strong>1</strong> — add one to the register</li>
</ul>
<p>Instruction bits are connected to control gates that determine
whether the adder output or zero is fed into the register. On each clock
cycle, the selected value is stored.</p>
<figure>
<img src="images/ch05-tiny-cpu.png"
alt="Tiny CPU datapath with instruction-controlled inputs" />
<figcaption aria-hidden="true">Tiny CPU datapath with
instruction-controlled inputs</figcaption>
</figure>
<p>Although extremely simple, this system demonstrates the core idea of
programmable behavior: control signals modify data paths on each
cycle.</p>
<hr />
<h2 id="why-memory-is-required">Why Memory Is Required</h2>
<p>Manual instruction selection is not sufficient for general
computation. Real programs require sequences of many instructions
executed automatically.</p>
<p>To support this, computers store instructions in memory and retrieve
them one by one during execution. This leads to the
<strong>fetch–decode–execute cycle</strong>:</p>
<ol type="1">
<li>fetch instruction from memory<br />
</li>
<li>decode instruction bits into control signals<br />
</li>
<li>execute operation<br />
</li>
<li>repeat</li>
</ol>
<p>This loop continues as long as the program runs.</p>
<figure>
<img src="images/ch05-fde-cycle.png"
alt="Fetch-decode-execute cycle diagram" />
<figcaption aria-hidden="true">Fetch-decode-execute cycle
diagram</figcaption>
</figure>
<hr />
<h2 id="program-counter-and-instruction-register">Program Counter and
Instruction Register</h2>
<p>Two special registers manage instruction sequencing:</p>
<ul>
<li>the <strong>Program Counter (PC)</strong> stores the address of the
next instruction<br />
</li>
<li>the <strong>Current Instruction Register (CIR)</strong> holds the
instruction being executed</li>
</ul>
<p>On each cycle, the PC advances, memory is accessed, and the CIR loads
the next instruction. Decoder circuits then examine the instruction bits
and activate the appropriate control signals for the ALU and
registers.</p>
<figure>
<img src="images/ch05-pc-cir.png"
alt="PC and CIR interaction with memory and control logic" />
<figcaption aria-hidden="true">PC and CIR interaction with memory and
control logic</figcaption>
</figure>
<hr />
<h2 id="decoding-instructions-with-gates">Decoding Instructions with
Gates</h2>
<p>Instruction decoding is performed using combinations of logic gates
called <strong>decoders</strong>. A decoder converts bit patterns into
individual control lines.</p>
<p>For example, if an instruction has three bits, a decoder can generate
eight distinct control signals, one for each possible instruction value.
These signals activate specific parts of the circuit for each
instruction type.</p>
<figure>
<img src="images/ch05-decoder.png"
alt="Binary decoder activating control lines" />
<figcaption aria-hidden="true">Binary decoder activating control
lines</figcaption>
</figure>
<p>This mechanism allows compact binary instructions to control large
physical systems.</p>
<hr />
<h2 id="abstraction-through-repetition">Abstraction Through
Repetition</h2>
<p>Although real processors contain billions of transistors, they are
composed of repeated versions of the same fundamental structures:</p>
<ul>
<li>adders<br />
</li>
<li>registers<br />
</li>
<li>multiplexers<br />
</li>
<li>decoders</li>
</ul>
<p>Complex behavior emerges from organized combinations of simple
components operating under synchronized timing.</p>
<p>This layered abstraction allows designers to reason about machines in
terms of data paths and control flows rather than individual
transistors.</p>
<hr />
<h2 id="summary-time-makes-programs-possible">Summary: Time Makes
Programs Possible</h2>
<p>Clocked circuits introduce controlled timing into digital systems,
allowing stable computation to occur in discrete steps. By separating
combinational logic from storage and coordinating updates with a clock,
reliable large-scale systems become possible.</p>
<p>Control logic and instruction decoding transform fixed circuits into
programmable machines. Registers, adders, and decoders cooperate to
execute instruction sequences automatically.</p>
<p>With clocked execution in place, computers can now run programs
rather than merely perform fixed calculations.</p>
<hr />
<h2 id="what-comes-next-4">What Comes Next</h2>
<p>The next chapter examines machine language and processor architecture
in more detail, using a small but complete instruction set inspired by
early microprocessors. Programs will be written directly in machine code
and executed in an emulator to reveal how software and hardware meet at
the lowest level.</p>
<h1 id="cpu-architecture-and-machine-code-how-programs-actually-run">CPU
Architecture and Machine Code: How Programs Actually Run</h1>
<p>Up to this point, digital systems have been built from logic gates,
storage elements, and clocked circuits. These components make it
possible to store values and perform arithmetic, but they do not yet
explain how a general-purpose machine can follow a sequence of
instructions. That capability emerges when computation is organized
around a central processing unit (CPU) that repeatedly executes a simple
control loop.</p>
<p>This chapter introduces a complete, working processor model and shows
how machine instructions drive its behavior. Programs are examined not
as abstract algorithms, but as concrete patterns of bits stored in
memory that directly control hardware.</p>
<hr />
<h2 id="from-fixed-circuits-to-programmable-machines">From Fixed
Circuits to Programmable Machines</h2>
<p>In earlier chapters, behavior was determined by wiring. If a circuit
always adds two numbers or always increments a counter, then its
function is fixed at design time. Programmable machines replace fixed
control paths with instruction-controlled behavior.</p>
<p>Instead of hardwiring what happens each cycle, instruction bits
select which operations occur and where data flows. The same physical
hardware can perform many different tasks simply by changing the
contents of memory.</p>
<p>This separation between hardware and program is the defining feature
of general-purpose computers.</p>
<hr />
<h2 id="a-simple-but-complete-cpu">A Simple but Complete CPU</h2>
<figure>
<img src="images/ch06-cpu-block-diagram.png"
alt="Block diagram of the CDC6504-style CPU" />
<figcaption aria-hidden="true">Block diagram of the CDC6504-style
CPU</figcaption>
</figure>
<p>A minimal but realistic processor can be built from the following
components:</p>
<ul>
<li><strong>Registers</strong> that store temporary values<br />
</li>
<li>a <strong>Program Counter (PC)</strong> that holds the address of
the next instruction<br />
</li>
<li>an <strong>Instruction Register (IR)</strong> that holds the current
instruction<br />
</li>
<li>an <strong>Arithmetic Logic Unit (ALU)</strong> that performs
arithmetic and logic<br />
</li>
<li><strong>Memory</strong> that stores both instructions and data<br />
</li>
<li><strong>Control logic</strong> that interprets instruction bits</li>
</ul>
<p>The CDC6504 emulator used in this book models a processor inspired by
early microprocessors, with a small number of registers and a compact
instruction set. Despite its simplicity, it contains all of the
essential elements found in modern CPUs.</p>
<hr />
<h2 id="registers-fast-small-storage">Registers: Fast, Small
Storage</h2>
<p>Registers are small storage locations located directly inside the
CPU. They are much faster to access than memory and are used to hold
intermediate results during computation.</p>
<p>Typical registers include:</p>
<ul>
<li>an <strong>Accumulator (A)</strong> for arithmetic results<br />
</li>
<li>index registers such as <strong>X</strong> and <strong>Y</strong>
for addressing and looping<br />
</li>
<li>a <strong>Status register</strong> that holds condition flags</li>
</ul>
<p>Flags record results of previous operations, such as whether a value
was zero or whether an arithmetic overflow occurred. Later instructions
can examine these flags to make decisions.</p>
<figure>
<img src="images/ch06-registers-flags.png"
alt="CPU register set and status flags" />
<figcaption aria-hidden="true">CPU register set and status
flags</figcaption>
</figure>
<hr />
<h2 id="memory-where-programs-and-data-live">Memory: Where Programs and
Data Live</h2>
<p>Memory stores both instructions and data as sequences of bytes. Each
byte is located at an address, and the Program Counter specifies which
address should be read next.</p>
<p>Although modern computers often separate instruction and data memory
internally, most early processors—and many simple designs—use a single
memory space for both. In such systems, instructions are simply data
that the CPU interprets in a special way.</p>
<p>This is why programs can be modified, copied, and even generated by
other programs.</p>
<hr />
<h2 id="the-fetchdecodeexecute-cycle">The Fetch–Decode–Execute
Cycle</h2>
<p>Every instruction executed by the CPU follows the same basic
sequence:</p>
<ol type="1">
<li><strong>Fetch</strong> the instruction from memory using the Program
Counter<br />
</li>
<li><strong>Decode</strong> the instruction to determine what operation
to perform<br />
</li>
<li><strong>Execute</strong> the operation using the ALU and
registers<br />
</li>
<li><strong>Update</strong> the Program Counter to the next
instruction</li>
</ol>
<p>This loop repeats continuously while the program runs.</p>
<figure>
<img src="images/ch06-fde-detailed.png"
alt="Fetch-decode-execute cycle with PC, IR, and control signals" />
<figcaption aria-hidden="true">Fetch-decode-execute cycle with PC, IR,
and control signals</figcaption>
</figure>
<p>Clock signals coordinate each stage so that values are stable when
they are stored or used.</p>
<hr />
<h2 id="why-hexadecimal-is-used">Why Hexadecimal Is Used</h2>
<p>Machine instructions are sequences of bits, but long binary strings
are difficult for humans to read. Hexadecimal notation groups bits into
sets of four, making memory contents easier to inspect and write.</p>
<p>For example:</p>
<ul>
<li>binary: <code>1010 1111</code><br />
</li>
<li>hex: <code>AF</code></li>
</ul>
<p>Each hexadecimal digit corresponds exactly to four binary bits. This
mapping makes it convenient to display memory as rows of hex values
while still representing precise machine data.</p>
<p>Assemblers, debuggers, and emulators commonly display memory using
hexadecimal for this reason.</p>
<figure>
<img src="images/ch06-hex-memory.png"
alt="Memory display showing hexadecimal values" />
<figcaption aria-hidden="true">Memory display showing hexadecimal
values</figcaption>
</figure>
<hr />
<h2 id="instruction-formats-and-operands">Instruction Formats and
Operands</h2>
<p>Each machine instruction contains:</p>
<ul>
<li>an <strong>opcode</strong> that specifies the operation<br />
</li>
<li>zero or more <strong>operands</strong> that specify data or
addresses</li>
</ul>
<p>Some instructions operate directly on registers, while others
reference memory. Common addressing modes include:</p>
<ul>
<li><strong>Immediate</strong>: value is part of the instruction<br />
</li>
<li><strong>Direct</strong>: instruction contains a memory address<br />
</li>
<li><strong>Indexed</strong>: address is computed using a register plus
an offset</li>
</ul>
<p>Different addressing modes allow programs to work with arrays,
tables, and strings efficiently.</p>
<figure>
<img src="images/ch06-instruction-format.png"
alt="Instruction format and addressing modes" />
<figcaption aria-hidden="true">Instruction format and addressing
modes</figcaption>
</figure>
<hr />
<h2 id="control-logic-and-decoding">Control Logic and Decoding</h2>
<p>Instruction decoding is performed by control logic that converts
opcode bits into control signals. These signals determine:</p>
<ul>
<li>which registers receive data<br />
</li>
<li>whether the ALU performs addition, subtraction, or comparison<br />
</li>
<li>whether memory is read or written<br />
</li>
<li>whether the Program Counter is modified</li>
</ul>
<p>Conceptually, decoding is a large decision tree built from logic
gates. In real processors, this logic is carefully optimized to minimize
delay along critical paths.</p>
<p>Although decoding may appear complex, it is simply a systematic
application of digital logic to route signals correctly.</p>
<hr />
<h2 id="sequential-flow-branches-and-loops">Sequential Flow, Branches,
and Loops</h2>
<p>By default, the Program Counter advances to the next instruction
after each cycle, causing instructions to execute sequentially. Branch
instructions modify the Program Counter based on conditions stored in
status flags.</p>
<p>This makes loops possible:</p>
<ol type="1">
<li>perform an operation<br />
</li>
<li>test a condition<br />
</li>
<li>branch back if the condition is not yet met</li>
</ol>
<p>All higher-level control structures—such as while loops and if
statements—ultimately reduce to conditional branches that alter the
Program Counter.</p>
<figure>
<img src="images/ch06-branch-flow.png"
alt="Branch instruction modifying program flow" />
<figcaption aria-hidden="true">Branch instruction modifying program
flow</figcaption>
</figure>
<hr />
<h2 id="example-counting-with-a-loop">Example: Counting with a Loop</h2>
<p>Consider a simple loop that increments a register until it reaches a
limit. In assembly language, this might look like:</p>
<pre class="asm"><code>LOAD A, #0
LOOP:
ADD  A, #1
CMP  A, #10
BNE  LOOP</code></pre>
<p>Each line corresponds to one or more machine instructions. During
execution, the Program Counter repeatedly jumps back to the label until
the condition is satisfied.</p>
<p>At the hardware level, this behavior is nothing more than controlled
updates to registers and the Program Counter on each clock cycle.</p>
<hr />
<h2 id="assembly-language-as-a-human-interface">Assembly Language as a
Human Interface</h2>
<p>Machine code is difficult to write directly. Assembly language
provides symbolic names for instructions and allows labels to represent
addresses. An <strong>assembler</strong> translates these symbolic
programs into machine code.</p>
<p>Assembly language does not add new capabilities to the machine. It
merely makes programs easier to write and understand.</p>
<figure>
<img src="images/ch06-assembler.png"
alt="Assembly source translated into machine code by an assembler" />
<figcaption aria-hidden="true">Assembly source translated into machine
code by an assembler</figcaption>
</figure>
<p>For educational purposes, writing small programs in assembly reveals
exactly how software controls hardware.</p>
<hr />
<h2 id="strings-characters-and-memory">Strings, Characters, and
Memory</h2>
<p>Characters are stored as numeric codes, such as ASCII values. A
string is simply a sequence of character codes stored in memory.</p>
<p>Programs that process text operate by:</p>
<ul>
<li>loading a character from memory<br />
</li>
<li>testing or modifying it<br />
</li>
<li>storing it back</li>
</ul>
<p>Operations such as converting letters to uppercase are performed by
arithmetic on character codes, not by any special text-handling
hardware.</p>
<p>This illustrates that all data—numbers, characters, images—are
treated uniformly as binary values by the processor.</p>
<hr />
<h2 id="from-python-to-machine-instructions">From Python to Machine
Instructions</h2>
<p>High-level languages such as Python hide hardware details, but their
execution still depends on machine instructions.</p>
<p>A loop written in Python becomes:</p>
<ul>
<li>comparisons<br />
</li>
<li>branches<br />
</li>
<li>register updates</li>
</ul>
<p>at the machine level. Although modern systems add layers such as
virtual machines and just-in-time compilation, the final execution
always reduces to instructions executed by hardware.</p>
<p>Understanding machine code provides insight into performance, memory
usage, and the real cost of software operations.</p>
<hr />
<h2 id="why-this-model-still-matters">Why This Model Still Matters</h2>
<p>Modern processors are far more complex than early microprocessors,
with multiple cores, deep pipelines, and sophisticated memory systems.
However, they still execute programs using the same fundamental
principles:</p>
<ul>
<li>fetch instructions<br />
</li>
<li>decode operations<br />
</li>
<li>manipulate registers and memory<br />
</li>
<li>update the Program Counter</li>
</ul>
<p>The complexity lies in doing many of these steps in parallel and at
extremely high speeds, not in changing the basic execution model.</p>
<p>Learning a small, complete CPU provides a foundation for
understanding much larger systems.</p>
<hr />
<h2 id="summary-programs-are-physical-processes">Summary: Programs Are
Physical Processes</h2>
<p>Programs are not abstract mathematical objects running in isolation.
They are physical processes enacted by electrical signals moving through
circuits, coordinated by clocks, and shaped by control logic.</p>
<p>Machine instructions directly control data paths and storage
elements. Assembly language offers a symbolic view of these
instructions, while high-level languages build additional layers of
abstraction on top.</p>
<p>By examining how a complete processor executes real programs, the
relationship between hardware and software becomes concrete and
observable.</p>
<hr />
<h2 id="what-comes-next-5">What Comes Next</h2>
<p>With a complete CPU model in place, attention can now turn to how
real-world processors improve performance through techniques such as
pipelining, caching, and parallel execution. The next chapter explores
how architectural enhancements build on the same foundations while
dramatically increasing speed.</p>
<h1
id="webassembly-and-emulation-running-real-programs-in-the-browser">WebAssembly
and Emulation: Running Real Programs in the Browser</h1>
<p>After building a complete mental model of how a CPU executes machine
instructions, it becomes possible to recognize the same ideas appearing
in unexpected places. One of the most surprising is the modern web
browser. Today, browsers include built‑in virtual machines capable of
executing low‑level binary code safely and efficiently.</p>
<p>This chapter explores how emulation and WebAssembly connect
historical machine architectures to modern execution environments, and
why the browser can now act as a practical platform for systems
programming.</p>
<hr />
<h2 id="from-hardware-to-emulators">From Hardware to Emulators</h2>
<p>An emulator is a program that imitates the behavior of a hardware
processor. Instead of electrical signals moving through gates, software
interprets instruction bytes and updates simulated registers and
memory.</p>
<p>At a high level, an emulator performs the same steps as real
hardware:</p>
<ol type="1">
<li>read the next instruction<br />
</li>
<li>decode the opcode<br />
</li>
<li>perform the operation<br />
</li>
<li>update registers and memory<br />
</li>
<li>advance the program counter</li>
</ol>
<p>This is the same fetch–decode–execute cycle implemented in
software.</p>
<figure>
<img src="images/ch07-emulator-loop.png"
alt="Emulator executing instructions in software" />
<figcaption aria-hidden="true">Emulator executing instructions in
software</figcaption>
</figure>
<p>Because modern computers are extraordinarily fast, they can often
emulate older machines faster than the original hardware ever ran.</p>
<hr />
<h2 id="why-emulation-is-practical-today">Why Emulation Is Practical
Today</h2>
<p>Early microprocessors such as the MOS 6502 contained only a few
thousand transistors and ran at clock speeds measured in megahertz.
Modern processors contain billions of transistors and operate at several
gigahertz.</p>
<p>This enormous performance gap means that:</p>
<ul>
<li>JavaScript running in a browser can emulate historical CPUs in real
time<br />
</li>
<li>graphics and sound can be simulated accurately<br />
</li>
<li>entire vintage game systems can be recreated in software</li>
</ul>
<p>Web sites such as the Internet Archive host playable emulations of
classic arcade machines that run entirely in the browser.</p>
<figure>
<img src="images/ch07-arcade-emulator.png"
alt="Classic game running in a browser-based emulator" />
<figcaption aria-hidden="true">Classic game running in a browser-based
emulator</figcaption>
</figure>
<hr />
<h2 id="the-cdc6504-emulator">The CDC6504 Emulator</h2>
<p>The CDC6504 emulator used in this course models a processor inspired
by the MOS 6502 instruction set with a simplified architecture. It
includes:</p>
<ul>
<li>registers (A, X, Y, status flags)<br />
</li>
<li>instruction memory<br />
</li>
<li>data memory<br />
</li>
<li>branching and arithmetic instructions</li>
</ul>
<p>Each instruction is implemented as a small block of JavaScript that
updates simulated hardware state.</p>
<p>Conceptually, each opcode becomes a function that performs the same
register and memory updates that real circuitry would perform in
silicon.</p>
<figure>
<img src="images/ch07-cdc6504-ui.png"
alt="Emulator showing registers, memory, and instruction pointer" />
<figcaption aria-hidden="true">Emulator showing registers, memory, and
instruction pointer</figcaption>
</figure>
<hr />
<h2 id="machine-code-inside-software">Machine Code Inside Software</h2>
<p>When an emulator runs, machine code is not special. It is simply a
sequence of numbers stored in an array. The emulator reads these numbers
and interprets them as instructions.</p>
<p>For example:</p>
<ul>
<li>a value may represent “load accumulator”<br />
</li>
<li>the next value may represent an address or constant<br />
</li>
<li>branching instructions modify the program counter</li>
</ul>
<p>The meaning comes entirely from how the emulator interprets the
bits.</p>
<p>This mirrors exactly what real hardware does, except that software
replaces physical wiring.</p>
<hr />
<h2 id="from-native-cpus-to-virtual-machines">From Native CPUs to
Virtual Machines</h2>
<p>Historically, assembly language targeted specific processors:</p>
<ul>
<li>MOS 6502<br />
</li>
<li>Intel x86<br />
</li>
<li>ARM</li>
</ul>
<p>Programs compiled for one architecture could not run on another
without translation or emulation.</p>
<p>WebAssembly changes this model by defining a portable virtual
instruction set that runs on top of browsers and other runtimes.</p>
<p>Instead of targeting physical hardware, compilers target a
standardized virtual machine.</p>
<hr />
<h2 id="what-is-webassembly">What Is WebAssembly?</h2>
<p>WebAssembly (WASM) is a low‑level, binary instruction format designed
for safe and efficient execution in browsers and other environments. It
is not tied to any specific physical CPU.</p>
<p>Key characteristics include:</p>
<ul>
<li>structured control flow<br />
</li>
<li>validated instruction sequences<br />
</li>
<li>sandboxed memory access<br />
</li>
<li>deterministic execution</li>
</ul>
<p>WASM programs cannot access files, devices, or the operating system
directly. All interaction with the outside world occurs through
carefully controlled interfaces provided by the host environment.</p>
<figure>
<img src="images/ch07-wasm-sandbox.png"
alt="WebAssembly execution sandbox inside the browser" />
<figcaption aria-hidden="true">WebAssembly execution sandbox inside the
browser</figcaption>
</figure>
<hr />
<h2 id="from-c-to-wasm">From C to WASM</h2>
<p>Languages such as C and C++ can be compiled to WebAssembly using
modern toolchains. The compilation process is:</p>
<ol type="1">
<li>source code is compiled to WASM bytecode<br />
</li>
<li>the browser loads and validates the module<br />
</li>
<li>the runtime translates WASM to native machine code<br />
</li>
<li>the program executes at near‑native speed</li>
</ol>
<p>From the browser’s perspective, WebAssembly is just another kind of
executable content, similar to JavaScript but closer to machine
operations.</p>
<hr />
<h2 id="a-wasm-hello-world">A WASM “Hello, World”</h2>
<p>A minimal WebAssembly program may look like this in textual form
(WAT):</p>
<pre class="wat"><code>(module
  (import &quot;console&quot; &quot;log&quot; (func $log (param i32 i32)))
  (memory 1)
  (data (i32.const 0) &quot;Hello, World!&quot;)
  (func $main (result i32)
    (call $log (i32.const 0) (i32.const 13))
    (i32.const 42)
  )
  (export &quot;main&quot; (func $main))
)</code></pre>
<p>This code:</p>
<ul>
<li>allocates memory<br />
</li>
<li>stores a string<br />
</li>
<li>calls a logging function<br />
</li>
<li>returns a numeric value</li>
</ul>
<p>When compiled, it becomes a compact binary format that the browser
can execute efficiently.</p>
<figure>
<img src="images/ch07-wasm-hex.png"
alt="Hex dump of compiled WebAssembly module" />
<figcaption aria-hidden="true">Hex dump of compiled WebAssembly
module</figcaption>
</figure>
<hr />
<h2 id="wasm-compared-to-traditional-assembly">WASM Compared to
Traditional Assembly</h2>
<p>Although WebAssembly resembles assembly language, it differs in
important ways:</p>
<ul>
<li><strong>Sandboxed by design</strong> — no direct memory or OS
access<br />
</li>
<li><strong>Safe execution model</strong> — code is validated before
running<br />
</li>
<li><strong>Portable bytecode</strong> — same program runs on any
platform<br />
</li>
<li><strong>Abstract machine</strong> — targets a virtual stack
machine<br />
</li>
<li><strong>Host optimization</strong> — browsers compile and optimize
at runtime</li>
</ul>
<p>Traditional assembly runs with full process privileges and depends
entirely on the operating system for protection. WASM embeds safety into
the execution model itself.</p>
<hr />
<h2 id="loops-functions-and-the-stack">Loops, Functions, and the
Stack</h2>
<p>Like physical CPUs, WebAssembly supports:</p>
<ul>
<li>local variables<br />
</li>
<li>function calls<br />
</li>
<li>loops and branches<br />
</li>
<li>a call stack</li>
</ul>
<p>When a function is called, parameters and return addresses are
managed using stack structures maintained by the runtime.</p>
<p>Even though the environment is virtual, the same concepts of control
flow and memory organization still apply.</p>
<hr />
<h2 id="from-historical-cpus-to-modern-devices">From Historical CPUs to
Modern Devices</h2>
<p>Over the last fifty years, many processor families have been
developed, but two now dominate consumer computing:</p>
<ul>
<li><strong>ARM</strong> processors, used in phones, tablets, and most
laptops<br />
</li>
<li><strong>x86</strong> processors, used in many desktop and server
systems</li>
</ul>
<p>These architectures differ internally, but they all implement the
same fundamental ideas explored in this book: registers, memory,
instructions, and controlled execution.</p>
<hr />
<h2 id="case-study-apples-cpu-transitions">Case Study: Apple’s CPU
Transitions</h2>
<p>Apple has moved across several processor families:</p>
<ul>
<li>MOS 6502 — early Apple computers<br />
</li>
<li>Motorola 68000 — early Macintosh systems<br />
</li>
<li>PowerPC — mid‑1990s through mid‑2000s<br />
</li>
<li>Intel x86 — 2006 through 2020<br />
</li>
<li>Apple‑designed ARM — 2020 to present</li>
</ul>
<p>To ease transitions, Apple built machine‑code translation systems
that converted programs from one architecture to another at launch time.
This allowed users to run older software while new native versions were
developed.</p>
<p>These transitions demonstrate that software compatibility can be
preserved even as hardware changes dramatically.</p>
<hr />
<h2 id="why-the-6502-still-matters">Why the 6502 Still Matters</h2>
<p>The design philosophy of early microprocessors influenced later
architectures. Engineers who designed early ARM processors had deep
experience with the 6502, and many principles carried forward into
modern instruction set design.</p>
<p>Although modern processors are vastly more complex, the core ideas
remain recognizable.</p>
<p>Understanding a small historical CPU therefore provides insight into
the design of modern systems.</p>
<hr />
<h2 id="emulation-as-a-learning-tool">Emulation as a Learning Tool</h2>
<p>Emulators allow direct observation of how instructions change machine
state:</p>
<ul>
<li>registers update<br />
</li>
<li>memory changes<br />
</li>
<li>branches alter execution flow</li>
</ul>
<p>This visibility is rarely available on modern hardware, where
pipelines and caches obscure internal behavior.</p>
<p>For learning computer architecture, emulation provides clarity that
real machines no longer expose.</p>
<hr />
<h2 id="summary-abstraction-without-losing-reality">Summary: Abstraction
Without Losing Reality</h2>
<p>WebAssembly and emulation demonstrate that low‑level computation
remains relevant even in modern software systems. Programs still execute
as sequences of instructions that manipulate memory and registers,
whether those instructions are interpreted, emulated, or executed
directly in silicon.</p>
<p>The same architectural principles govern both historical processors
and modern virtual machines. Only the layers of abstraction have
changed.</p>
<p>By tracing the path from physical circuits to browser‑based
execution, the continuity of computer architecture becomes clear.</p>
<hr />
<h2 id="closing-thoughts">Closing Thoughts</h2>
<p>Computers have evolved enormously in speed and scale, but not in
fundamental design. Logic gates became processors, processors became
systems, and systems became virtual machines running inside
browsers.</p>
<p>Understanding how computation works at the lowest level makes it
possible to see through layers of abstraction and recognize the same
mechanisms at work everywhere—from embedded devices to cloud servers to
web pages running in a browser.</p>
<p>This perspective provides both practical insight and a deeper
appreciation for the remarkable continuity of computing technology.</p>
</body>
</html>
